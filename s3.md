# S3

- Max size 5TB.
- Buckets are regional.
- Strong read after write consistency.

# Bucket Policies and ACLs

When you apply the bucket owner enforced setting for Object Ownership to disable ACLs, you automatically own and take full control over every object in the bucket without taking any additional actions. After you apply this setting there are 3 changes:

- All bucket ACLs and object ACLs are disabled, giving full access to you as the bucket owner. When you perform a read ACL request on your bucket or object you'll see that full access is given only to the bucket owner.
- You, as the bucket owner, automatically own and have full control over every object in your bucket.
- AACLs no longer affect access permissions to your bucket. As a result, access control for your data is based on policies, such as IAM policies, S3 bucket policies, VPC endpoint policies and Org. SCPs.


## CMK policy


````
{
  "Version": "2012-10-17",
  "Id": "key-policy-1",
  "Statement": [
    {
      "Sid": "GetPut",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::ExampleBucket/*"
    },
    {
      "Sid": "KMS",
      "Effect": "Allow",
      "Action": [
        "kms:Decrypt",
        "kms:GenerateDataKey",
        "kms:Encrypt"
      ],
      "Resource": "arn:aws:kms:us-west-1:111122223333:key/keyid-12345"
    }
  ]
}
````

kms:GenerateDataKey

GenerateDataKey returns a unique symmetric data key for use outside of AWS KMS. This operation returns a plaintext copy of the data key and a copy that is encrypted under a symmetric encryption KMS key that you specify. The bytes in the plaintext key are random; they are not related to the caller or the KMS key. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.

## Sub-selecting data from objects - first 250 Bytes of an object
https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html

### Byte Range fetch

Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in ElasticSearch

Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.

### S3 Select Scan Ranges
With Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte). You can then store the relevant information in the form of a JSON document in ElasticSearch.

### Supported standard tiering
````
Standard RR -> Standard IA -> Intelligent Tiering-> One Zone IA -> S3 Glacier-> S3 Glacier Deep Archive
````

### Encryption

- You can use Amazon S3 batch operations to encrypt existing unencrypted S3 objects or perform cross-bucket copies.
- You can also encrypt existing objects by using the CopyObject API operation or the copy-object AWS CLI command.
- Enforce encryption with a bucket policy ````s3:x-amz-server-side-encryption: aws:kms````

### S3 Presigned URLs

- ```` aws s3 presign s3://bobbinsbucket/cool_image.jpg" ````
- URL returned with an expiry date. URL by default expires in 1 hour.

### S3 Web Server Access Logging

- Provides detailed records for the requests that are made to a bucket.
- Details include requester, bucket name, request time, request action, response status and error code.
- Disabled by default.
- Must configure a separate bucket as the destination.
- Must grant write permissions to the S3 log deliveryt group on the destination bucket.

### S3 Event Notification

- Create an SNS topic
- Create a subscription - email
- Confirm the subscription in email
- Add an Access policy for the SNS queue.
- S3 event notification needs to be allowed SNS:Publish
- Create event notification in S3.
- Destinations: Lambda, SNS, SQS.

### S3 query from Athena

- Store data in an optimized columnar format such as Parquet or ORC
- Partition the data in S3 using Apache Hive partitioning filename contains datestamp. Us a date column as a partition key.

### S3 Select
Makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. 
S3 Select simplifies and improves the performance of scanning and filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational investigations on log files in Amazon S3 without the need to operate or manage a compute cluster.
S3 Select and S3 Athena can perform the same, but S3 Select is cheaper and feature limited.

### VPC Endpoints for S3

|Gateway endpoints for Amazon S3|Interface endpoints for Amazon S3|
|---|---|
|Traffic remains on the AWS network.|Traffic remains on the AWS network.|
|Use Amazon S3 public IP addresses|Use private IP addresses from your VPC to access Amazon S3|
|Use the same Amazon S3 DNS names|Require endpoint-specific Amazon S3 DNS names|
|Do not allow access from on premises|Allow access from on premises|
|Do not allow access from another AWS Region|Allow access from a VPC in another AWS Region by using VPC peering or AWS Transit Gateway|
|Not billed|Billed|

### Transfer Acceleration

Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50%-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion, and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications. S3TA improves transfer performance by routing traffic through Amazon CloudFrontâ€™s globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations. You can turn on S3TA with a few clicks in the S3 console, and test its benefits from your location with a speed comparison tool. With S3TA, you pay only for transfers that are accelerated.

### S3 Versioning

- First version of a file upload has a version ID of Null.
- Second version of a file uploaded has a version ID of 1. 


You can configure S3 Versioning for an existing S3 bucket, but you *can only configure Object Lock*  when you first create a bucket. You cannot configure Object Lock for a bucket that already exists.

### S3 Intelligent Tiering

https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html

- *Frequent Access tier (automatic):* This is the default access tier that any object created or transitioned to S3 Intelligent-Tiering begins its lifecycle in. An object remains in this tier as long as it is being accessed. The Frequent Access tier provides low latency and high-throughput performance.
- *Infrequent Access tier (automatic):* If an object is not accessed for 30 consecutive days, the object moves to the Infrequent Access tier. The Infrequent Access tier provides low latency and high-throughput performance.
- *Archive Instant Access tier (automatic):* If an object is not accessed for 90 consecutive days, the object moves to the Archive Instant Access tier. The Archive Instant Access tier provides low latency and high-throughput performance.
- *Archive Access tier (optional):* S3 Intelligent-Tiering provides you with the option to activate the Archive Access tier for data that can be accessed asynchronously. After activation, the Archive Access tier automatically archives objects that have not been accessed for a minimum of 90 consecutive days. You can extend the last access time for archiving to a maximum of 730 days. The Archive Access tier has the same performance as the S3 Glacier Flexible Retrieval storage class.
- *Deep Archive Access tier (optional):* S3 Intelligent-Tiering provides you with the option to activate the Deep Archive Access tier for data that can be accessed asynchronously. After activation, the Deep Archive Access tier automatically archives objects that have not been accessed for a minimum of 180 consecutive days. You can extend the last access time for archiving to a maximum of 730 days. The Deep Archive Access tier has the same performance as the S3 Glacier Deep Archive storage class.